# -*- coding: utf-8 -*-
"""word frequency for asa 2025

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TfiAPAjboma31sMR0qYIZPccQVtJ81ne

# word frequency for asa 2025
"""

#importing our csv file
from google.colab import files
uploaded = files.upload()

#All questions


#'Topic of Interest'
#'What are some types of tasks that you would like to assist with or aspects you are looking forward to?'
#'What type of skills do you possess (ex. Photoshop, languages)?'
#'Describe in 4-5 sentences what your three learning goals are for the internship Program.'
#'Describe your educational and career plans. What do you hope to achieve in the future?'

import pandas as pd

#read the file, but we need extra steps since this csv file is odd
try:
    df = pd.read_csv('asa.csv.csv', encoding='utf-8')
except UnicodeDecodeError:
    df = pd.read_csv('asa.csv.csv', encoding='latin-1')

#replacing all the \n, which mean "next line" with a comma
df = df.replace(r'\n', ', ', regex=True)

#See all the columns here
#print("columns found in the dataframe")
#print(df.columns)

#See what types are the columns
#print(df.dtypes)

#print the whole dataframe
#print(df.head())
#print(df)

#'describe your educational and career plans what do you hope to achieve in the future'
#'describe in 4-5 sentences what your three learning goals are for the internship program'
#'what type of skills do you possess?'
#'what are some types of tasks that you would like to assist with or aspects you are looking forward to'

#Describe your educational and career plans. What do you hope to achieve in the future?

#put everything into one large string
text = df['Describe your educational and career plans. What do you hope to achieve in the future?'].str.cat(sep=' ')

#import all
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Download necessary resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
# Download the punkt_tab resource explicitly
nltk.download('punkt_tab')

# Tokenization
tokens = word_tokenize(text)

# Lowercasing
tokens = [token.lower() for token in tokens]

# Stop word removal
stop_words = set(stopwords.words('english'))
tokens = [token for token in tokens if token not in stop_words]

# Punctuation removal
tokens = [token for token in tokens if token.isalnum()]

# Lemmatization
lemmatizer = WordNetLemmatizer()
tokens = [lemmatizer.lemmatize(token) for token in tokens]

# Print results
#print("Preprocessed Tokens:", tokens)

sentence = 'describe your educational and career plans what do you hope to achieve in the future field path create change better life either study major make focus high pursue attend help work aim want goal get like would also able plan school become'
words = sentence.split()

#Counting the words in the list of token words
word_counts_dict = {}
for i in tokens:
  if i not in words:
    if i not in word_counts_dict:
      word_counts_dict[i]= 1
    else:
      word_counts_dict[i] +=1

#print final
one = dict(sorted(word_counts_dict.items(), key=lambda item: item[1], reverse=True))
print(one)

#Describe in 4-5 sentences what your three learning goals are for the internship Program.

#put everything into one large string
text = df['Describe in 4-5 sentences what your three learning goals are for the internship Program.'].str.cat(sep=' ')

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Download necessary resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
# Download the punkt_tab resource explicitly
nltk.download('punkt_tab')

# Tokenization
tokens = word_tokenize(text)

# Lowercasing
tokens = [token.lower() for token in tokens]

# Stop word removal
stop_words = set(stopwords.words('english'))
tokens = [token for token in tokens if token not in stop_words]

# Punctuation removal
tokens = [token for token in tokens if token.isalnum()]

# Lemmatization
lemmatizer = WordNetLemmatizer()
tokens = [lemmatizer.lemmatize(token) for token in tokens]

# Print results
#print("Preprocessed Tokens:", tokens)

sentence = 'describe in 4-5 sentences what your three learning goals are for the internship program allow around well get one hope goal skill want would also like lastly working able aim first'
words = sentence.split()

#Counting the words in the list of token words
word_counts_dict = {}
for i in tokens:
  if i not in words:
    if i not in word_counts_dict:
      word_counts_dict[i]= 1
    else:
      word_counts_dict[i] +=1

two = dict(sorted(word_counts_dict.items(), key=lambda item: item[1], reverse=True))
print(two)

#What type of skills do you possess (ex. Photoshop, languages)?

#put everything into one large string
text = df['What type of skills do you possess (ex. Photoshop, languages)?'].str.cat(sep=' ')

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Download necessary resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
# Download the punkt_tab resource explicitly
nltk.download('punkt_tab')

# Tokenization
tokens = word_tokenize(text)

# Lowercasing
tokens = [token.lower() for token in tokens]

# Stop word removal
stop_words = set(stopwords.words('english'))
tokens = [token for token in tokens if token not in stop_words]

# Punctuation removal
tokens = [token for token in tokens if token.isalnum()]

# Lemmatization
lemmatizer = WordNetLemmatizer()
tokens = [lemmatizer.lemmatize(token) for token in tokens]

# Print results
#print("Preprocessed Tokens:", tokens)

sentence = 'what type of skills do you possess skill also additionally well good currently year like many ability able help experience strong school help effectively proficient across various'
words = sentence.split()

#Counting the words in the list of token words
word_counts_dict = {}
for i in tokens:
  if i not in words:
    if i not in word_counts_dict:
      word_counts_dict[i]= 1
    else:
      word_counts_dict[i] +=1

three = dict(sorted(word_counts_dict.items(), key=lambda item: item[1], reverse=True))
print(three)

#What are some types of tasks that you would like to assist with or aspects you are looking forward to?

#put everything into one large string
text = df['What are some types of tasks that you would like to assist with or aspects you are looking forward to?'].str.cat(sep=' ')

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Download necessary resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
# Download the punkt_tab resource explicitly
nltk.download('punkt_tab')

# Tokenization
tokens = word_tokenize(text)

# Lowercasing
tokens = [token.lower() for token in tokens]

# Stop word removal
stop_words = set(stopwords.words('english'))
tokens = [token for token in tokens if token not in stop_words]

# Punctuation removal
tokens = [token for token in tokens if token.isalnum()]

# Lemmatization
lemmatizer = WordNetLemmatizer()
tokens = [lemmatizer.lemmatize(token) for token in tokens]

# Print results
#print("Preprocessed Tokens:", tokens)

sentence = 'what are some types of tasks that you would like to assist with or aspects you are looking forward to task make working excited help also work aspect helping able contributing want look'
words = sentence.split()

#Counting the words in the list of token words
word_counts_dict = {}
for i in tokens:
  if i not in words:
    if i not in word_counts_dict:
      word_counts_dict[i]= 1
    else:
      word_counts_dict[i] +=1

four = dict(sorted(word_counts_dict.items(), key=lambda item: item[1], reverse=True))
print(four)

#Topic of Interest
five = df['Topic of Interest'].str.split(',\s*').explode().value_counts()
print(five)

oneword = "Describe your educational and career plans. What do you hope to achieve in the future?"
twoword = 'Describe in 4-5 sentences what your three learning goals are for the internship Program.'
threeword = 'What type of skills do you possess (ex. Photoshop, languages)?'
fourword ='What are some types of tasks that you would like to assist with or aspects you are looking forward to?'
fiveword ='Topic of Interest'

print("Describe your educational and career plans. What do you hope to achieve in the future? \n", one, '\n')
print('Describe in 4-5 sentences what your three learning goals are for the internship Program. \n', two, '\n')
print('What type of skills do you possess (ex. Photoshop, languages)? \n',  three, '\n')
print('What are some types of tasks that you would like to assist with or aspects you are looking forward to? \n', four, '\n')
print('Topic of Interest \n', five)

#taking a dictionary and making a bar graph out of it

data_dict= one

key = []
value = []
n = 10

# Iterate through the items of the dictionary
for index, (k, v) in enumerate(data_dict.items()):
  # Limit the number of items plotted to 'n'
  if index == n:
    break
  else:
    key.append(k)  # Append the actual key (word)
    value.append(v) # Append the actual value (frequency)

import matplotlib.pyplot as plt

plt.bar(key, value, color = 'black')
#plt.xlabel("word", fontsize=14)
plt.ylabel("word frequency", fontsize=14)
plt.title(oneword, fontsize=16)
plt.tick_params(axis='x', labelsize=14)
plt.tick_params(axis='y', labelsize=14)
ax = plt.gca()
ax.tick_params(axis='x', rotation=90)
plt.show()

#taking a dictionary and making a bar graph out of it

data_dict= two

key = []
value = []
n = 10

# Iterate through the items of the dictionary
for index, (k, v) in enumerate(data_dict.items()):
  # Limit the number of items plotted to 'n'
  if index == n:
    break
  else:
    key.append(k)  # Append the actual key (word)
    value.append(v) # Append the actual value (frequency)

import matplotlib.pyplot as plt

plt.bar(key, value, color = 'black')
#plt.xlabel("word", fontsize=14)
plt.ylabel("word frequency", fontsize=14)
plt.title(twoword, fontsize=16)
plt.tick_params(axis='x', labelsize=14)
plt.tick_params(axis='y', labelsize=14)
ax = plt.gca()
ax.tick_params(axis='x', rotation=90)
plt.show()

#taking a dictionary and making a bar graph out of it

data_dict= three

key = []
value = []
n = 10

# Iterate through the items of the dictionary
for index, (k, v) in enumerate(data_dict.items()):
  # Limit the number of items plotted to 'n'
  if index == n:
    break
  else:
    key.append(k)  # Append the actual key (word)
    value.append(v) # Append the actual value (frequency)

import matplotlib.pyplot as plt

plt.bar(key, value, color = 'black')
#plt.xlabel("word", fontsize=14)
plt.ylabel("word frequency", fontsize=14)
plt.title(threeword, fontsize=16)
plt.tick_params(axis='x', labelsize=14)
plt.tick_params(axis='y', labelsize=14)
ax = plt.gca()
ax.tick_params(axis='x', rotation=90)
plt.show()

#taking a dictionary and making a bar graph out of it

data_dict= four

key = []
value = []
n = 10

# Iterate through the items of the dictionary
for index, (k, v) in enumerate(data_dict.items()):
  # Limit the number of items plotted to 'n'
  if index == n:
    break
  else:
    key.append(k)  # Append the actual key (word)
    value.append(v) # Append the actual value (frequency)

import matplotlib.pyplot as plt

plt.bar(key, value, color = 'black')
#plt.xlabel("word", fontsize=14)
plt.ylabel("word frequency", fontsize=14)
plt.title(fourword, fontsize=16)
plt.tick_params(axis='x', labelsize=14)
plt.tick_params(axis='y', labelsize=14)
ax = plt.gca()
ax.tick_params(axis='x', rotation=90)
plt.show()

key = ['Global Health','Community Development', 'Environmental Sustainability', 'Children & Education', 'Any topic']
value = [43,41,38,35,25]

import matplotlib.pyplot as plt

plt.bar(key, value, color = 'black')
#plt.xlabel("word", fontsize=14)
plt.ylabel("word frequency", fontsize=14)
plt.title(fiveword, fontsize=16)
plt.tick_params(axis='x', labelsize=14)
plt.tick_params(axis='y', labelsize=14)
ax = plt.gca()
ax.tick_params(axis='x', rotation=90)
plt.show()